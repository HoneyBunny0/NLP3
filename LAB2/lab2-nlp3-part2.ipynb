{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MastafaF/Lab2_EPITA/blob/main/Lab2_EPITA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMWTa9Vnep8a",
        "tags": []
      },
      "source": [
        "# II. NLP across languages, a multilingual setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3daMllJP75s"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/HoneyBunny0/NLP3/raw/main/LAB2/MUSE.zip\n",
        "!unzip MUSE.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTpFvw3KepHF",
        "outputId": "00e7963f-8a75-4c0a-bc13-8967b548f30f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2886M  100 2886M    0     0  14.0M      0  0:03:24  0:03:24 --:--:-- 11.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 6291M  100 6291M    0     0  13.9M      0  0:07:32  0:07:32 --:--:-- 16.7M\n"
          ]
        }
      ],
      "source": [
        "!curl -Lo MUSE/data/wiki.fr.vec https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.fr.vec\n",
        "!curl -Lo MUSE/data/wiki.en.vec https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtV4BcGjZIzy",
        "outputId": "6aabf849-1aac-4c2e-92d1-347a13f628b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9iQc2JlPNStF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41a1657-511a-4621-9264-23ead481c367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MUSE\n"
          ]
        }
      ],
      "source": [
        "%cd MUSE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with wikipedias's datas"
      ],
      "metadata": {
        "id": "IIfnkUw-06TN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axnhkt5ESckl",
        "outputId": "3fa675e0-22ae-4ea3-c954-37fd1ec52890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO - 01/11/23 22:24:49 - 0:00:00 - ============ Initialized logger ============\n",
            "INFO - 01/11/23 22:24:49 - 0:00:00 - adversarial: True\n",
            "                                     batch_size: 32\n",
            "                                     cuda: True\n",
            "                                     dico_build: S2T\n",
            "                                     dico_eval: default\n",
            "                                     dico_max_rank: 15000\n",
            "                                     dico_max_size: 0\n",
            "                                     dico_method: csls_knn_10\n",
            "                                     dico_min_size: 0\n",
            "                                     dico_threshold: 0\n",
            "                                     dis_clip_weights: 0\n",
            "                                     dis_dropout: 0.0\n",
            "                                     dis_hid_dim: 2048\n",
            "                                     dis_input_dropout: 0.1\n",
            "                                     dis_lambda: 1\n",
            "                                     dis_layers: 2\n",
            "                                     dis_most_frequent: 75000\n",
            "                                     dis_optimizer: sgd,lr=0.1\n",
            "                                     dis_smooth: 0.1\n",
            "                                     dis_steps: 5\n",
            "                                     emb_dim: 300\n",
            "                                     epoch_size: 1000000\n",
            "                                     exp_id: \n",
            "                                     exp_name: fr_en\n",
            "                                     exp_path: ./fr_en/r7aymiv3do\n",
            "                                     export: \n",
            "                                     lr_decay: 0.98\n",
            "                                     lr_shrink: 0.5\n",
            "                                     map_beta: 0.001\n",
            "                                     map_id_init: True\n",
            "                                     map_optimizer: sgd,lr=0.1\n",
            "                                     max_vocab: 200000\n",
            "                                     min_lr: 1e-06\n",
            "                                     n_epochs: 1\n",
            "                                     n_refinement: 5\n",
            "                                     normalize_embeddings: \n",
            "                                     seed: -1\n",
            "                                     src_emb: data/wiki.fr.vec\n",
            "                                     src_lang: fr\n",
            "                                     tgt_emb: data/wiki.en.vec\n",
            "                                     tgt_lang: en\n",
            "                                     verbose: 2\n",
            "INFO - 01/11/23 22:24:49 - 0:00:00 - The experiment will be stored in ./fr_en/r7aymiv3do\n",
            "INFO - 01/11/23 22:25:05 - 0:00:16 - Loaded 200000 pre-trained word embeddings.\n",
            "INFO - 01/11/23 22:25:17 - 0:00:29 - Loaded 200000 pre-trained word embeddings.\n",
            "INFO - 01/11/23 22:25:19 - 0:00:31 - ----> ADVERSARIAL TRAINING <----\n",
            "                                     \n",
            "                                     \n",
            "INFO - 01/11/23 22:25:19 - 0:00:31 - Starting adversarial training epoch 0...\n",
            "/content/MUSE/src/trainer.py:69: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  src_emb = self.src_emb(Variable(src_ids, volatile=True))\n",
            "/content/MUSE/src/trainer.py:70: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  tgt_emb = self.tgt_emb(Variable(tgt_ids, volatile=True))\n",
            "/content/MUSE/src/trainer.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  src_emb = self.mapping(Variable(src_emb.data, volatile=volatile))\n",
            "/content/MUSE/src/trainer.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  tgt_emb = Variable(tgt_emb.data, volatile=volatile)\n",
            "INFO - 01/11/23 22:25:20 - 0:00:31 - 000000 - Discriminator loss: 0.6802 - 111 samples/s\n",
            "INFO - 01/11/23 22:25:21 - 0:00:33 - 004000 - Discriminator loss: 0.6150 - 5385 samples/s\n",
            "INFO - 01/11/23 22:25:23 - 0:00:34 - 008000 - Discriminator loss: 0.5158 - 5644 samples/s\n",
            "INFO - 01/11/23 22:25:24 - 0:00:35 - 012000 - Discriminator loss: 0.5477 - 5578 samples/s\n",
            "INFO - 01/11/23 22:25:25 - 0:00:37 - 016000 - Discriminator loss: 0.5643 - 5579 samples/s\n",
            "INFO - 01/11/23 22:25:27 - 0:00:38 - 020000 - Discriminator loss: 0.5534 - 5547 samples/s\n",
            "INFO - 01/11/23 22:25:28 - 0:00:40 - 024000 - Discriminator loss: 0.5365 - 5706 samples/s\n",
            "INFO - 01/11/23 22:25:30 - 0:00:41 - 028000 - Discriminator loss: 0.5178 - 5633 samples/s\n",
            "INFO - 01/11/23 22:25:31 - 0:00:43 - 032000 - Discriminator loss: 0.4993 - 5590 samples/s\n",
            "INFO - 01/11/23 22:25:33 - 0:00:44 - 036000 - Discriminator loss: 0.4945 - 5625 samples/s\n",
            "INFO - 01/11/23 22:25:34 - 0:00:45 - 040000 - Discriminator loss: 0.4877 - 5664 samples/s\n",
            "INFO - 01/11/23 22:25:35 - 0:00:47 - 044000 - Discriminator loss: 0.4803 - 5669 samples/s\n",
            "INFO - 01/11/23 22:25:37 - 0:00:48 - 048000 - Discriminator loss: 0.4786 - 5703 samples/s\n",
            "INFO - 01/11/23 22:25:38 - 0:00:50 - 052000 - Discriminator loss: 0.4702 - 5674 samples/s\n",
            "INFO - 01/11/23 22:25:40 - 0:00:51 - 056000 - Discriminator loss: 0.4679 - 5682 samples/s\n",
            "INFO - 01/11/23 22:25:41 - 0:00:52 - 060000 - Discriminator loss: 0.4618 - 5667 samples/s\n",
            "INFO - 01/11/23 22:25:42 - 0:00:54 - 064000 - Discriminator loss: 0.4587 - 5567 samples/s\n",
            "INFO - 01/11/23 22:25:44 - 0:00:55 - 068000 - Discriminator loss: 0.4515 - 5736 samples/s\n",
            "INFO - 01/11/23 22:25:45 - 0:00:57 - 072000 - Discriminator loss: 0.4554 - 5660 samples/s\n",
            "INFO - 01/11/23 22:25:47 - 0:00:58 - 076000 - Discriminator loss: 0.4535 - 5460 samples/s\n",
            "INFO - 01/11/23 22:25:48 - 0:01:00 - 080000 - Discriminator loss: 0.4479 - 5521 samples/s\n",
            "INFO - 01/11/23 22:25:50 - 0:01:01 - 084000 - Discriminator loss: 0.4468 - 5679 samples/s\n",
            "INFO - 01/11/23 22:25:51 - 0:01:02 - 088000 - Discriminator loss: 0.4422 - 5600 samples/s\n",
            "INFO - 01/11/23 22:25:52 - 0:01:04 - 092000 - Discriminator loss: 0.4396 - 5600 samples/s\n",
            "INFO - 01/11/23 22:25:54 - 0:01:05 - 096000 - Discriminator loss: 0.4378 - 5564 samples/s\n",
            "INFO - 01/11/23 22:25:55 - 0:01:07 - 100000 - Discriminator loss: 0.4409 - 5456 samples/s\n",
            "INFO - 01/11/23 22:25:57 - 0:01:08 - 104000 - Discriminator loss: 0.4310 - 5592 samples/s\n",
            "INFO - 01/11/23 22:25:58 - 0:01:10 - 108000 - Discriminator loss: 0.4356 - 5592 samples/s\n",
            "INFO - 01/11/23 22:26:00 - 0:01:11 - 112000 - Discriminator loss: 0.4355 - 5530 samples/s\n",
            "INFO - 01/11/23 22:26:01 - 0:01:13 - 116000 - Discriminator loss: 0.4300 - 5603 samples/s\n",
            "INFO - 01/11/23 22:26:02 - 0:01:14 - 120000 - Discriminator loss: 0.4299 - 5671 samples/s\n",
            "INFO - 01/11/23 22:26:04 - 0:01:15 - 124000 - Discriminator loss: 0.4308 - 5737 samples/s\n",
            "INFO - 01/11/23 22:26:05 - 0:01:17 - 128000 - Discriminator loss: 0.4284 - 5646 samples/s\n",
            "INFO - 01/11/23 22:26:07 - 0:01:18 - 132000 - Discriminator loss: 0.4260 - 5605 samples/s\n",
            "INFO - 01/11/23 22:26:08 - 0:01:20 - 136000 - Discriminator loss: 0.4271 - 5617 samples/s\n",
            "INFO - 01/11/23 22:26:10 - 0:01:21 - 140000 - Discriminator loss: 0.4256 - 5506 samples/s\n",
            "INFO - 01/11/23 22:26:11 - 0:01:23 - 144000 - Discriminator loss: 0.4274 - 5016 samples/s\n",
            "INFO - 01/11/23 22:26:13 - 0:01:24 - 148000 - Discriminator loss: 0.4236 - 4939 samples/s\n",
            "INFO - 01/11/23 22:26:14 - 0:01:26 - 152000 - Discriminator loss: 0.4279 - 5497 samples/s\n",
            "INFO - 01/11/23 22:26:16 - 0:01:27 - 156000 - Discriminator loss: 0.4260 - 5528 samples/s\n",
            "INFO - 01/11/23 22:26:17 - 0:01:29 - 160000 - Discriminator loss: 0.4274 - 5433 samples/s\n",
            "INFO - 01/11/23 22:26:19 - 0:01:30 - 164000 - Discriminator loss: 0.4247 - 5563 samples/s\n",
            "INFO - 01/11/23 22:26:20 - 0:01:32 - 168000 - Discriminator loss: 0.4260 - 5650 samples/s\n",
            "INFO - 01/11/23 22:26:21 - 0:01:33 - 172000 - Discriminator loss: 0.4258 - 5594 samples/s\n",
            "INFO - 01/11/23 22:26:23 - 0:01:34 - 176000 - Discriminator loss: 0.4234 - 5549 samples/s\n",
            "INFO - 01/11/23 22:26:24 - 0:01:36 - 180000 - Discriminator loss: 0.4234 - 5578 samples/s\n",
            "INFO - 01/11/23 22:26:26 - 0:01:37 - 184000 - Discriminator loss: 0.4270 - 5648 samples/s\n",
            "INFO - 01/11/23 22:26:27 - 0:01:39 - 188000 - Discriminator loss: 0.4285 - 5671 samples/s\n",
            "INFO - 01/11/23 22:26:29 - 0:01:40 - 192000 - Discriminator loss: 0.4289 - 5672 samples/s\n",
            "INFO - 01/11/23 22:26:30 - 0:01:41 - 196000 - Discriminator loss: 0.4265 - 5674 samples/s\n",
            "INFO - 01/11/23 22:26:31 - 0:01:43 - 200000 - Discriminator loss: 0.4306 - 5584 samples/s\n",
            "INFO - 01/11/23 22:26:33 - 0:01:44 - 204000 - Discriminator loss: 0.4278 - 5566 samples/s\n",
            "INFO - 01/11/23 22:26:34 - 0:01:46 - 208000 - Discriminator loss: 0.4307 - 5536 samples/s\n",
            "INFO - 01/11/23 22:26:36 - 0:01:48 - 212000 - Discriminator loss: 0.4333 - 4596 samples/s\n",
            "INFO - 01/11/23 22:26:38 - 0:01:50 - 216000 - Discriminator loss: 0.4353 - 3931 samples/s\n",
            "INFO - 01/11/23 22:26:40 - 0:01:51 - 220000 - Discriminator loss: 0.4321 - 5489 samples/s\n",
            "INFO - 01/11/23 22:26:41 - 0:01:52 - 224000 - Discriminator loss: 0.4352 - 5665 samples/s\n",
            "INFO - 01/11/23 22:26:42 - 0:01:54 - 228000 - Discriminator loss: 0.4362 - 5537 samples/s\n",
            "INFO - 01/11/23 22:26:44 - 0:01:55 - 232000 - Discriminator loss: 0.4331 - 5550 samples/s\n",
            "INFO - 01/11/23 22:26:45 - 0:01:57 - 236000 - Discriminator loss: 0.4368 - 5478 samples/s\n",
            "INFO - 01/11/23 22:26:47 - 0:01:58 - 240000 - Discriminator loss: 0.4404 - 5495 samples/s\n",
            "INFO - 01/11/23 22:26:48 - 0:02:00 - 244000 - Discriminator loss: 0.4397 - 5485 samples/s\n",
            "INFO - 01/11/23 22:26:50 - 0:02:01 - 248000 - Discriminator loss: 0.4397 - 5467 samples/s\n",
            "INFO - 01/11/23 22:26:51 - 0:02:03 - 252000 - Discriminator loss: 0.4428 - 5511 samples/s\n",
            "INFO - 01/11/23 22:26:53 - 0:02:04 - 256000 - Discriminator loss: 0.4388 - 5317 samples/s\n",
            "INFO - 01/11/23 22:26:54 - 0:02:06 - 260000 - Discriminator loss: 0.4395 - 5693 samples/s\n",
            "INFO - 01/11/23 22:26:55 - 0:02:07 - 264000 - Discriminator loss: 0.4421 - 5553 samples/s\n",
            "INFO - 01/11/23 22:26:57 - 0:02:08 - 268000 - Discriminator loss: 0.4419 - 5566 samples/s\n",
            "INFO - 01/11/23 22:26:58 - 0:02:10 - 272000 - Discriminator loss: 0.4435 - 5602 samples/s\n",
            "INFO - 01/11/23 22:27:00 - 0:02:11 - 276000 - Discriminator loss: 0.4416 - 5496 samples/s\n",
            "INFO - 01/11/23 22:27:01 - 0:02:13 - 280000 - Discriminator loss: 0.4413 - 5690 samples/s\n",
            "INFO - 01/11/23 22:27:03 - 0:02:14 - 284000 - Discriminator loss: 0.4410 - 5599 samples/s\n",
            "INFO - 01/11/23 22:27:04 - 0:02:16 - 288000 - Discriminator loss: 0.4436 - 5667 samples/s\n",
            "INFO - 01/11/23 22:27:06 - 0:02:17 - 292000 - Discriminator loss: 0.4422 - 5478 samples/s\n",
            "INFO - 01/11/23 22:27:07 - 0:02:18 - 296000 - Discriminator loss: 0.4431 - 5493 samples/s\n",
            "INFO - 01/11/23 22:27:08 - 0:02:20 - 300000 - Discriminator loss: 0.4399 - 5410 samples/s\n",
            "INFO - 01/11/23 22:27:10 - 0:02:21 - 304000 - Discriminator loss: 0.4400 - 5339 samples/s\n",
            "INFO - 01/11/23 22:27:12 - 0:02:24 - 308000 - Discriminator loss: 0.4430 - 3411 samples/s\n",
            "INFO - 01/11/23 22:27:15 - 0:02:26 - 312000 - Discriminator loss: 0.4401 - 3175 samples/s\n",
            "INFO - 01/11/23 22:27:16 - 0:02:28 - 316000 - Discriminator loss: 0.4360 - 5335 samples/s\n",
            "INFO - 01/11/23 22:27:18 - 0:02:29 - 320000 - Discriminator loss: 0.4397 - 5608 samples/s\n",
            "INFO - 01/11/23 22:27:19 - 0:02:31 - 324000 - Discriminator loss: 0.4409 - 5521 samples/s\n",
            "INFO - 01/11/23 22:27:21 - 0:02:32 - 328000 - Discriminator loss: 0.4399 - 5495 samples/s\n",
            "INFO - 01/11/23 22:27:22 - 0:02:34 - 332000 - Discriminator loss: 0.4407 - 4766 samples/s\n",
            "INFO - 01/11/23 22:27:24 - 0:02:35 - 336000 - Discriminator loss: 0.4380 - 5290 samples/s\n",
            "INFO - 01/11/23 22:27:25 - 0:02:37 - 340000 - Discriminator loss: 0.4363 - 5580 samples/s\n",
            "INFO - 01/11/23 22:27:27 - 0:02:38 - 344000 - Discriminator loss: 0.4334 - 5613 samples/s\n",
            "INFO - 01/11/23 22:27:28 - 0:02:40 - 348000 - Discriminator loss: 0.4393 - 5579 samples/s\n",
            "INFO - 01/11/23 22:27:30 - 0:02:41 - 352000 - Discriminator loss: 0.4349 - 5634 samples/s\n",
            "INFO - 01/11/23 22:27:31 - 0:02:42 - 356000 - Discriminator loss: 0.4357 - 5527 samples/s\n",
            "INFO - 01/11/23 22:27:32 - 0:02:44 - 360000 - Discriminator loss: 0.4375 - 5570 samples/s\n",
            "INFO - 01/11/23 22:27:34 - 0:02:45 - 364000 - Discriminator loss: 0.4317 - 5679 samples/s\n",
            "INFO - 01/11/23 22:27:35 - 0:02:47 - 368000 - Discriminator loss: 0.4329 - 5568 samples/s\n",
            "INFO - 01/11/23 22:27:37 - 0:02:48 - 372000 - Discriminator loss: 0.4341 - 4835 samples/s\n",
            "INFO - 01/11/23 22:27:39 - 0:02:51 - 376000 - Discriminator loss: 0.4339 - 3446 samples/s\n",
            "INFO - 01/11/23 22:27:41 - 0:02:53 - 380000 - Discriminator loss: 0.4336 - 4122 samples/s\n",
            "INFO - 01/11/23 22:27:44 - 0:02:55 - 384000 - Discriminator loss: 0.4316 - 3320 samples/s\n",
            "INFO - 01/11/23 22:27:45 - 0:02:57 - 388000 - Discriminator loss: 0.4363 - 5472 samples/s\n",
            "INFO - 01/11/23 22:27:47 - 0:02:58 - 392000 - Discriminator loss: 0.4327 - 5543 samples/s\n",
            "INFO - 01/11/23 22:27:48 - 0:02:59 - 396000 - Discriminator loss: 0.4363 - 5372 samples/s\n",
            "INFO - 01/11/23 22:27:49 - 0:03:01 - 400000 - Discriminator loss: 0.4355 - 5570 samples/s\n",
            "INFO - 01/11/23 22:27:51 - 0:03:02 - 404000 - Discriminator loss: 0.4362 - 5618 samples/s\n",
            "INFO - 01/11/23 22:27:52 - 0:03:04 - 408000 - Discriminator loss: 0.4384 - 5606 samples/s\n",
            "INFO - 01/11/23 22:27:54 - 0:03:05 - 412000 - Discriminator loss: 0.4364 - 5609 samples/s\n",
            "INFO - 01/11/23 22:27:55 - 0:03:07 - 416000 - Discriminator loss: 0.4390 - 5569 samples/s\n",
            "INFO - 01/11/23 22:27:57 - 0:03:08 - 420000 - Discriminator loss: 0.4382 - 5558 samples/s\n",
            "INFO - 01/11/23 22:27:58 - 0:03:09 - 424000 - Discriminator loss: 0.4352 - 5511 samples/s\n",
            "INFO - 01/11/23 22:28:00 - 0:03:11 - 428000 - Discriminator loss: 0.4354 - 5422 samples/s\n",
            "INFO - 01/11/23 22:28:01 - 0:03:12 - 432000 - Discriminator loss: 0.4354 - 5571 samples/s\n",
            "INFO - 01/11/23 22:28:02 - 0:03:14 - 436000 - Discriminator loss: 0.4349 - 5620 samples/s\n",
            "INFO - 01/11/23 22:28:04 - 0:03:15 - 440000 - Discriminator loss: 0.4353 - 5540 samples/s\n",
            "INFO - 01/11/23 22:28:05 - 0:03:17 - 444000 - Discriminator loss: 0.4349 - 5629 samples/s\n",
            "INFO - 01/11/23 22:28:07 - 0:03:18 - 448000 - Discriminator loss: 0.4328 - 5612 samples/s\n",
            "INFO - 01/11/23 22:28:08 - 0:03:20 - 452000 - Discriminator loss: 0.4314 - 5626 samples/s\n",
            "INFO - 01/11/23 22:28:10 - 0:03:21 - 456000 - Discriminator loss: 0.4302 - 5537 samples/s\n",
            "INFO - 01/11/23 22:28:11 - 0:03:22 - 460000 - Discriminator loss: 0.4276 - 5548 samples/s\n",
            "INFO - 01/11/23 22:28:12 - 0:03:24 - 464000 - Discriminator loss: 0.4290 - 5581 samples/s\n",
            "INFO - 01/11/23 22:28:14 - 0:03:25 - 468000 - Discriminator loss: 0.4300 - 5565 samples/s\n",
            "INFO - 01/11/23 22:28:15 - 0:03:27 - 472000 - Discriminator loss: 0.4281 - 5575 samples/s\n",
            "INFO - 01/11/23 22:28:17 - 0:03:28 - 476000 - Discriminator loss: 0.4290 - 5589 samples/s\n",
            "INFO - 01/11/23 22:28:19 - 0:03:30 - 480000 - Discriminator loss: 0.4294 - 4060 samples/s\n",
            "INFO - 01/11/23 22:28:20 - 0:03:32 - 484000 - Discriminator loss: 0.4278 - 5523 samples/s\n",
            "INFO - 01/11/23 22:28:22 - 0:03:33 - 488000 - Discriminator loss: 0.4275 - 5478 samples/s\n",
            "INFO - 01/11/23 22:28:23 - 0:03:34 - 492000 - Discriminator loss: 0.4307 - 5553 samples/s\n",
            "INFO - 01/11/23 22:28:24 - 0:03:36 - 496000 - Discriminator loss: 0.4265 - 5493 samples/s\n",
            "INFO - 01/11/23 22:28:26 - 0:03:37 - 500000 - Discriminator loss: 0.4288 - 5616 samples/s\n",
            "INFO - 01/11/23 22:28:27 - 0:03:39 - 504000 - Discriminator loss: 0.4266 - 5522 samples/s\n",
            "INFO - 01/11/23 22:28:29 - 0:03:40 - 508000 - Discriminator loss: 0.4253 - 5493 samples/s\n",
            "INFO - 01/11/23 22:28:30 - 0:03:42 - 512000 - Discriminator loss: 0.4242 - 5525 samples/s\n",
            "INFO - 01/11/23 22:28:32 - 0:03:43 - 516000 - Discriminator loss: 0.4244 - 5405 samples/s\n",
            "INFO - 01/11/23 22:28:33 - 0:03:45 - 520000 - Discriminator loss: 0.4206 - 5625 samples/s\n",
            "INFO - 01/11/23 22:28:35 - 0:03:46 - 524000 - Discriminator loss: 0.4229 - 5651 samples/s\n",
            "INFO - 01/11/23 22:28:36 - 0:03:48 - 528000 - Discriminator loss: 0.4222 - 5349 samples/s\n",
            "INFO - 01/11/23 22:28:39 - 0:03:50 - 532000 - Discriminator loss: 0.4223 - 3106 samples/s\n",
            "INFO - 01/11/23 22:28:41 - 0:03:52 - 536000 - Discriminator loss: 0.4187 - 3402 samples/s\n",
            "INFO - 01/11/23 22:28:43 - 0:03:54 - 540000 - Discriminator loss: 0.4171 - 4609 samples/s\n",
            "INFO - 01/11/23 22:28:44 - 0:03:56 - 544000 - Discriminator loss: 0.4176 - 5678 samples/s\n",
            "INFO - 01/11/23 22:28:46 - 0:03:57 - 548000 - Discriminator loss: 0.4203 - 5709 samples/s\n",
            "INFO - 01/11/23 22:28:47 - 0:03:58 - 552000 - Discriminator loss: 0.4166 - 5606 samples/s\n",
            "INFO - 01/11/23 22:28:48 - 0:04:00 - 556000 - Discriminator loss: 0.4179 - 5496 samples/s\n",
            "INFO - 01/11/23 22:28:50 - 0:04:01 - 560000 - Discriminator loss: 0.4167 - 5617 samples/s\n",
            "INFO - 01/11/23 22:28:51 - 0:04:03 - 564000 - Discriminator loss: 0.4182 - 5159 samples/s\n",
            "INFO - 01/11/23 22:28:53 - 0:04:04 - 568000 - Discriminator loss: 0.4163 - 5465 samples/s\n",
            "INFO - 01/11/23 22:28:54 - 0:04:06 - 572000 - Discriminator loss: 0.4159 - 5518 samples/s\n",
            "INFO - 01/11/23 22:28:56 - 0:04:07 - 576000 - Discriminator loss: 0.4128 - 5642 samples/s\n",
            "INFO - 01/11/23 22:28:57 - 0:04:09 - 580000 - Discriminator loss: 0.4128 - 5579 samples/s\n",
            "INFO - 01/11/23 22:28:59 - 0:04:10 - 584000 - Discriminator loss: 0.4143 - 5741 samples/s\n",
            "INFO - 01/11/23 22:29:00 - 0:04:11 - 588000 - Discriminator loss: 0.4125 - 5609 samples/s\n",
            "INFO - 01/11/23 22:29:01 - 0:04:13 - 592000 - Discriminator loss: 0.4127 - 5506 samples/s\n",
            "INFO - 01/11/23 22:29:03 - 0:04:14 - 596000 - Discriminator loss: 0.4097 - 5564 samples/s\n",
            "INFO - 01/11/23 22:29:04 - 0:04:16 - 600000 - Discriminator loss: 0.4105 - 5552 samples/s\n",
            "INFO - 01/11/23 22:29:06 - 0:04:17 - 604000 - Discriminator loss: 0.4079 - 5664 samples/s\n",
            "INFO - 01/11/23 22:29:07 - 0:04:19 - 608000 - Discriminator loss: 0.4099 - 5689 samples/s\n",
            "INFO - 01/11/23 22:29:09 - 0:04:20 - 612000 - Discriminator loss: 0.4084 - 5595 samples/s\n",
            "INFO - 01/11/23 22:29:10 - 0:04:21 - 616000 - Discriminator loss: 0.4057 - 5640 samples/s\n",
            "INFO - 01/11/23 22:29:11 - 0:04:23 - 620000 - Discriminator loss: 0.4080 - 5663 samples/s\n",
            "INFO - 01/11/23 22:29:13 - 0:04:24 - 624000 - Discriminator loss: 0.4069 - 5520 samples/s\n",
            "INFO - 01/11/23 22:29:14 - 0:04:26 - 628000 - Discriminator loss: 0.4065 - 5617 samples/s\n",
            "INFO - 01/11/23 22:29:16 - 0:04:27 - 632000 - Discriminator loss: 0.4064 - 5589 samples/s\n",
            "INFO - 01/11/23 22:29:17 - 0:04:29 - 636000 - Discriminator loss: 0.4058 - 5586 samples/s\n",
            "INFO - 01/11/23 22:29:19 - 0:04:30 - 640000 - Discriminator loss: 0.4040 - 5612 samples/s\n",
            "INFO - 01/11/23 22:29:20 - 0:04:31 - 644000 - Discriminator loss: 0.4033 - 5541 samples/s\n",
            "INFO - 01/11/23 22:29:21 - 0:04:33 - 648000 - Discriminator loss: 0.4026 - 5528 samples/s\n",
            "INFO - 01/11/23 22:29:23 - 0:04:34 - 652000 - Discriminator loss: 0.4030 - 5456 samples/s\n",
            "INFO - 01/11/23 22:29:24 - 0:04:36 - 656000 - Discriminator loss: 0.4033 - 5542 samples/s\n",
            "INFO - 01/11/23 22:29:26 - 0:04:37 - 660000 - Discriminator loss: 0.4034 - 5570 samples/s\n",
            "INFO - 01/11/23 22:29:27 - 0:04:39 - 664000 - Discriminator loss: 0.4028 - 5594 samples/s\n",
            "INFO - 01/11/23 22:29:29 - 0:04:40 - 668000 - Discriminator loss: 0.4027 - 5592 samples/s\n",
            "INFO - 01/11/23 22:29:30 - 0:04:42 - 672000 - Discriminator loss: 0.3998 - 5601 samples/s\n",
            "INFO - 01/11/23 22:29:32 - 0:04:43 - 676000 - Discriminator loss: 0.4009 - 5603 samples/s\n",
            "INFO - 01/11/23 22:29:33 - 0:04:44 - 680000 - Discriminator loss: 0.4021 - 5550 samples/s\n",
            "INFO - 01/11/23 22:29:34 - 0:04:46 - 684000 - Discriminator loss: 0.3997 - 5576 samples/s\n",
            "INFO - 01/11/23 22:29:36 - 0:04:47 - 688000 - Discriminator loss: 0.3991 - 5542 samples/s\n",
            "INFO - 01/11/23 22:29:37 - 0:04:49 - 692000 - Discriminator loss: 0.3983 - 5651 samples/s\n",
            "INFO - 01/11/23 22:29:39 - 0:04:50 - 696000 - Discriminator loss: 0.3970 - 5682 samples/s\n",
            "INFO - 01/11/23 22:29:40 - 0:04:52 - 700000 - Discriminator loss: 0.3987 - 5645 samples/s\n",
            "INFO - 01/11/23 22:29:42 - 0:04:53 - 704000 - Discriminator loss: 0.3972 - 5620 samples/s\n",
            "INFO - 01/11/23 22:29:43 - 0:04:54 - 708000 - Discriminator loss: 0.3968 - 5609 samples/s\n",
            "INFO - 01/11/23 22:29:44 - 0:04:56 - 712000 - Discriminator loss: 0.3955 - 5627 samples/s\n",
            "INFO - 01/11/23 22:29:46 - 0:04:57 - 716000 - Discriminator loss: 0.3970 - 5559 samples/s\n",
            "INFO - 01/11/23 22:29:47 - 0:04:59 - 720000 - Discriminator loss: 0.3965 - 5051 samples/s\n",
            "INFO - 01/11/23 22:29:49 - 0:05:00 - 724000 - Discriminator loss: 0.3955 - 5032 samples/s\n",
            "INFO - 01/11/23 22:29:50 - 0:05:02 - 728000 - Discriminator loss: 0.3972 - 5611 samples/s\n",
            "INFO - 01/11/23 22:29:52 - 0:05:03 - 732000 - Discriminator loss: 0.3972 - 5666 samples/s\n",
            "INFO - 01/11/23 22:29:53 - 0:05:05 - 736000 - Discriminator loss: 0.3988 - 5479 samples/s\n",
            "INFO - 01/11/23 22:29:55 - 0:05:06 - 740000 - Discriminator loss: 0.3957 - 5611 samples/s\n",
            "INFO - 01/11/23 22:29:56 - 0:05:08 - 744000 - Discriminator loss: 0.3968 - 5569 samples/s\n",
            "INFO - 01/11/23 22:29:58 - 0:05:09 - 748000 - Discriminator loss: 0.3968 - 5601 samples/s\n",
            "INFO - 01/11/23 22:29:59 - 0:05:10 - 752000 - Discriminator loss: 0.3963 - 5626 samples/s\n",
            "INFO - 01/11/23 22:30:00 - 0:05:12 - 756000 - Discriminator loss: 0.3954 - 5495 samples/s\n",
            "INFO - 01/11/23 22:30:02 - 0:05:13 - 760000 - Discriminator loss: 0.3965 - 5591 samples/s\n",
            "INFO - 01/11/23 22:30:03 - 0:05:15 - 764000 - Discriminator loss: 0.3960 - 5592 samples/s\n",
            "INFO - 01/11/23 22:30:05 - 0:05:16 - 768000 - Discriminator loss: 0.3957 - 5682 samples/s\n",
            "INFO - 01/11/23 22:30:06 - 0:05:18 - 772000 - Discriminator loss: 0.3956 - 5681 samples/s\n",
            "INFO - 01/11/23 22:30:08 - 0:05:19 - 776000 - Discriminator loss: 0.3954 - 5545 samples/s\n",
            "INFO - 01/11/23 22:30:09 - 0:05:20 - 780000 - Discriminator loss: 0.3941 - 5653 samples/s\n",
            "INFO - 01/11/23 22:30:10 - 0:05:22 - 784000 - Discriminator loss: 0.3940 - 5504 samples/s\n",
            "INFO - 01/11/23 22:30:12 - 0:05:23 - 788000 - Discriminator loss: 0.3929 - 5478 samples/s\n",
            "INFO - 01/11/23 22:30:13 - 0:05:25 - 792000 - Discriminator loss: 0.3920 - 5526 samples/s\n",
            "INFO - 01/11/23 22:30:15 - 0:05:26 - 796000 - Discriminator loss: 0.3923 - 5552 samples/s\n",
            "INFO - 01/11/23 22:30:16 - 0:05:28 - 800000 - Discriminator loss: 0.3932 - 5438 samples/s\n",
            "INFO - 01/11/23 22:30:18 - 0:05:29 - 804000 - Discriminator loss: 0.3925 - 5472 samples/s\n",
            "INFO - 01/11/23 22:30:19 - 0:05:31 - 808000 - Discriminator loss: 0.3913 - 5601 samples/s\n",
            "INFO - 01/11/23 22:30:21 - 0:05:32 - 812000 - Discriminator loss: 0.3920 - 5669 samples/s\n",
            "INFO - 01/11/23 22:30:22 - 0:05:33 - 816000 - Discriminator loss: 0.3928 - 5492 samples/s\n",
            "INFO - 01/11/23 22:30:23 - 0:05:35 - 820000 - Discriminator loss: 0.3922 - 5488 samples/s\n",
            "INFO - 01/11/23 22:30:25 - 0:05:36 - 824000 - Discriminator loss: 0.3936 - 5657 samples/s\n",
            "INFO - 01/11/23 22:30:26 - 0:05:38 - 828000 - Discriminator loss: 0.3916 - 5567 samples/s\n",
            "INFO - 01/11/23 22:30:28 - 0:05:39 - 832000 - Discriminator loss: 0.3928 - 5471 samples/s\n",
            "INFO - 01/11/23 22:30:29 - 0:05:41 - 836000 - Discriminator loss: 0.3911 - 5494 samples/s\n",
            "INFO - 01/11/23 22:30:31 - 0:05:42 - 840000 - Discriminator loss: 0.3908 - 5606 samples/s\n",
            "INFO - 01/11/23 22:30:32 - 0:05:44 - 844000 - Discriminator loss: 0.3901 - 5596 samples/s\n",
            "INFO - 01/11/23 22:30:34 - 0:05:45 - 848000 - Discriminator loss: 0.3903 - 5547 samples/s\n",
            "INFO - 01/11/23 22:30:35 - 0:05:46 - 852000 - Discriminator loss: 0.3881 - 5590 samples/s\n",
            "INFO - 01/11/23 22:30:36 - 0:05:48 - 856000 - Discriminator loss: 0.3904 - 5530 samples/s\n",
            "INFO - 01/11/23 22:30:38 - 0:05:49 - 860000 - Discriminator loss: 0.3897 - 5453 samples/s\n",
            "INFO - 01/11/23 22:30:39 - 0:05:51 - 864000 - Discriminator loss: 0.3879 - 5484 samples/s\n",
            "INFO - 01/11/23 22:30:41 - 0:05:52 - 868000 - Discriminator loss: 0.3898 - 5512 samples/s\n",
            "INFO - 01/11/23 22:30:42 - 0:05:54 - 872000 - Discriminator loss: 0.3859 - 5543 samples/s\n",
            "INFO - 01/11/23 22:30:44 - 0:05:55 - 876000 - Discriminator loss: 0.3871 - 5620 samples/s\n",
            "INFO - 01/11/23 22:30:45 - 0:05:57 - 880000 - Discriminator loss: 0.3861 - 5669 samples/s\n",
            "INFO - 01/11/23 22:30:47 - 0:05:58 - 884000 - Discriminator loss: 0.3848 - 5594 samples/s\n",
            "INFO - 01/11/23 22:30:48 - 0:05:59 - 888000 - Discriminator loss: 0.3852 - 5622 samples/s\n",
            "INFO - 01/11/23 22:30:49 - 0:06:01 - 892000 - Discriminator loss: 0.3861 - 5563 samples/s\n",
            "INFO - 01/11/23 22:30:51 - 0:06:02 - 896000 - Discriminator loss: 0.3862 - 5632 samples/s\n",
            "INFO - 01/11/23 22:30:52 - 0:06:04 - 900000 - Discriminator loss: 0.3855 - 5558 samples/s\n",
            "INFO - 01/11/23 22:30:54 - 0:06:05 - 904000 - Discriminator loss: 0.3860 - 5565 samples/s\n",
            "INFO - 01/11/23 22:30:55 - 0:06:07 - 908000 - Discriminator loss: 0.3854 - 5600 samples/s\n",
            "INFO - 01/11/23 22:30:57 - 0:06:08 - 912000 - Discriminator loss: 0.3840 - 5647 samples/s\n",
            "INFO - 01/11/23 22:30:58 - 0:06:09 - 916000 - Discriminator loss: 0.3832 - 5679 samples/s\n",
            "INFO - 01/11/23 22:30:59 - 0:06:11 - 920000 - Discriminator loss: 0.3845 - 5598 samples/s\n",
            "INFO - 01/11/23 22:31:01 - 0:06:12 - 924000 - Discriminator loss: 0.3847 - 5500 samples/s\n",
            "INFO - 01/11/23 22:31:02 - 0:06:14 - 928000 - Discriminator loss: 0.3839 - 5078 samples/s\n",
            "INFO - 01/11/23 22:31:04 - 0:06:15 - 932000 - Discriminator loss: 0.3840 - 5012 samples/s\n",
            "INFO - 01/11/23 22:31:05 - 0:06:17 - 936000 - Discriminator loss: 0.3834 - 5638 samples/s\n",
            "INFO - 01/11/23 22:31:07 - 0:06:18 - 940000 - Discriminator loss: 0.3821 - 5553 samples/s\n",
            "INFO - 01/11/23 22:31:08 - 0:06:20 - 944000 - Discriminator loss: 0.3813 - 5557 samples/s\n",
            "INFO - 01/11/23 22:31:10 - 0:06:21 - 948000 - Discriminator loss: 0.3832 - 5593 samples/s\n",
            "INFO - 01/11/23 22:31:11 - 0:06:23 - 952000 - Discriminator loss: 0.3827 - 5569 samples/s\n",
            "INFO - 01/11/23 22:31:13 - 0:06:24 - 956000 - Discriminator loss: 0.3826 - 5628 samples/s\n",
            "INFO - 01/11/23 22:31:14 - 0:06:25 - 960000 - Discriminator loss: 0.3822 - 5531 samples/s\n",
            "INFO - 01/11/23 22:31:15 - 0:06:27 - 964000 - Discriminator loss: 0.3815 - 5635 samples/s\n",
            "INFO - 01/11/23 22:31:17 - 0:06:28 - 968000 - Discriminator loss: 0.3830 - 5461 samples/s\n",
            "INFO - 01/11/23 22:31:18 - 0:06:30 - 972000 - Discriminator loss: 0.3824 - 5449 samples/s\n",
            "INFO - 01/11/23 22:31:20 - 0:06:31 - 976000 - Discriminator loss: 0.3824 - 5588 samples/s\n",
            "INFO - 01/11/23 22:31:21 - 0:06:33 - 980000 - Discriminator loss: 0.3833 - 5534 samples/s\n",
            "INFO - 01/11/23 22:31:23 - 0:06:34 - 984000 - Discriminator loss: 0.3811 - 5567 samples/s\n",
            "INFO - 01/11/23 22:31:24 - 0:06:36 - 988000 - Discriminator loss: 0.3821 - 4728 samples/s\n",
            "INFO - 01/11/23 22:31:26 - 0:06:38 - 992000 - Discriminator loss: 0.3833 - 3795 samples/s\n",
            "INFO - 01/11/23 22:31:28 - 0:06:39 - 996000 - Discriminator loss: 0.3825 - 5546 samples/s\n",
            "INFO - 01/11/23 22:31:33 - 0:06:44 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:31:33 - 0:06:44 - New train dictionary of 6299 pairs.\n",
            "INFO - 01/11/23 22:31:33 - 0:06:44 - Mean cosine (nn method, S2T build, 10000 max size): 0.61856\n",
            "INFO - 01/11/23 22:32:09 - 0:07:21 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:32:09 - 0:07:21 - New train dictionary of 6790 pairs.\n",
            "INFO - 01/11/23 22:32:09 - 0:07:21 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.60812\n",
            "/content/MUSE/src/evaluation/evaluator.py:232: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  emb = Variable(self.src_emb.weight[i:i + bs].data, volatile=True)\n",
            "/content/MUSE/src/evaluation/evaluator.py:237: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  emb = Variable(self.tgt_emb.weight[i:i + bs].data, volatile=True)\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - Discriminator source / target predictions: 0.82297 / 0.25201\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - Discriminator source / target / global accuracy: 0.95941 / 0.81746 / 0.88844\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - __log__:{\"n_epoch\": 0, \"mean_cosine-nn-S2T-10000\": 0.6185649633407593, \"mean_cosine-csls_knn_10-S2T-10000\": 0.6081181168556213, \"dis_accu\": 0.8884375, \"dis_src_pred\": 0.8229700005594548, \"dis_tgt_pred\": 0.25201319499921415}\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - * Best value for \"mean_cosine-csls_knn_10-S2T-10000\": 0.60812\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - * Saving the mapping to ./fr_en/r7aymiv3do/best_mapping.pth ...\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - End of epoch 0.\n",
            "                                     \n",
            "                                     \n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - Decreasing learning rate: 0.10000000 -> 0.09800000\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - ----> ITERATIVE PROCRUSTES REFINEMENT <----\n",
            "                                     \n",
            "                                     \n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - * Reloading the best model from ./fr_en/r7aymiv3do/best_mapping.pth ...\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - Starting refinement iteration 0...\n",
            "INFO - 01/11/23 22:32:11 - 0:07:22 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:32:30 - 0:07:41 - New train dictionary of 10296 pairs.\n",
            "INFO - 01/11/23 22:32:33 - 0:07:45 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:32:33 - 0:07:45 - New train dictionary of 7380 pairs.\n",
            "INFO - 01/11/23 22:32:33 - 0:07:45 - Mean cosine (nn method, S2T build, 10000 max size): 0.70381\n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - New train dictionary of 7343 pairs.\n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.70139\n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - __log__:{\"n_iter\": 0, \"mean_cosine-nn-S2T-10000\": 0.7038137912750244, \"mean_cosine-csls_knn_10-S2T-10000\": 0.7013887763023376}\n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - * Best value for \"mean_cosine-csls_knn_10-S2T-10000\": 0.70139\n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - * Saving the mapping to ./fr_en/r7aymiv3do/best_mapping.pth ...\n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - End of refinement iteration 0.\n",
            "                                     \n",
            "                                     \n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - Starting refinement iteration 1...\n",
            "INFO - 01/11/23 22:33:09 - 0:08:21 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:33:28 - 0:08:39 - New train dictionary of 11246 pairs.\n",
            "INFO - 01/11/23 22:33:31 - 0:08:43 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:33:31 - 0:08:43 - New train dictionary of 7452 pairs.\n",
            "INFO - 01/11/23 22:33:31 - 0:08:43 - Mean cosine (nn method, S2T build, 10000 max size): 0.70575\n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - New train dictionary of 7404 pairs.\n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.70410\n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - __log__:{\"n_iter\": 1, \"mean_cosine-nn-S2T-10000\": 0.7057475447654724, \"mean_cosine-csls_knn_10-S2T-10000\": 0.7040963172912598}\n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - * Best value for \"mean_cosine-csls_knn_10-S2T-10000\": 0.70410\n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - * Saving the mapping to ./fr_en/r7aymiv3do/best_mapping.pth ...\n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - End of refinement iteration 1.\n",
            "                                     \n",
            "                                     \n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - Starting refinement iteration 2...\n",
            "INFO - 01/11/23 22:34:08 - 0:09:19 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:34:27 - 0:09:38 - New train dictionary of 11355 pairs.\n",
            "INFO - 01/11/23 22:34:30 - 0:09:42 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:34:30 - 0:09:42 - New train dictionary of 7477 pairs.\n",
            "INFO - 01/11/23 22:34:30 - 0:09:42 - Mean cosine (nn method, S2T build, 10000 max size): 0.70587\n",
            "INFO - 01/11/23 22:35:06 - 0:10:18 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:35:06 - 0:10:18 - New train dictionary of 7443 pairs.\n",
            "INFO - 01/11/23 22:35:06 - 0:10:18 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.70388\n",
            "INFO - 01/11/23 22:35:06 - 0:10:18 - __log__:{\"n_iter\": 2, \"mean_cosine-nn-S2T-10000\": 0.7058731913566589, \"mean_cosine-csls_knn_10-S2T-10000\": 0.7038848400115967}\n",
            "INFO - 01/11/23 22:35:06 - 0:10:18 - End of refinement iteration 2.\n",
            "                                     \n",
            "                                     \n",
            "INFO - 01/11/23 22:35:06 - 0:10:18 - Starting refinement iteration 3...\n",
            "INFO - 01/11/23 22:35:06 - 0:10:18 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:35:25 - 0:10:36 - New train dictionary of 11400 pairs.\n",
            "INFO - 01/11/23 22:35:28 - 0:10:40 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:35:28 - 0:10:40 - New train dictionary of 7498 pairs.\n",
            "INFO - 01/11/23 22:35:28 - 0:10:40 - Mean cosine (nn method, S2T build, 10000 max size): 0.70580\n",
            "INFO - 01/11/23 22:36:05 - 0:11:16 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:36:05 - 0:11:16 - New train dictionary of 7469 pairs.\n",
            "INFO - 01/11/23 22:36:05 - 0:11:16 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.70376\n",
            "INFO - 01/11/23 22:36:05 - 0:11:16 - __log__:{\"n_iter\": 3, \"mean_cosine-nn-S2T-10000\": 0.7058048844337463, \"mean_cosine-csls_knn_10-S2T-10000\": 0.7037554383277893}\n",
            "INFO - 01/11/23 22:36:05 - 0:11:16 - End of refinement iteration 3.\n",
            "                                     \n",
            "                                     \n",
            "INFO - 01/11/23 22:36:05 - 0:11:16 - Starting refinement iteration 4...\n",
            "INFO - 01/11/23 22:36:05 - 0:11:16 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:36:23 - 0:11:35 - New train dictionary of 11425 pairs.\n",
            "INFO - 01/11/23 22:36:27 - 0:11:38 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:36:27 - 0:11:38 - New train dictionary of 7530 pairs.\n",
            "INFO - 01/11/23 22:36:27 - 0:11:38 - Mean cosine (nn method, S2T build, 10000 max size): 0.70540\n",
            "INFO - 01/11/23 22:37:04 - 0:12:15 - Building the train dictionary ...\n",
            "INFO - 01/11/23 22:37:04 - 0:12:15 - New train dictionary of 7497 pairs.\n",
            "INFO - 01/11/23 22:37:04 - 0:12:15 - Mean cosine (csls_knn_10 method, S2T build, 10000 max size): 0.70371\n",
            "INFO - 01/11/23 22:37:04 - 0:12:15 - __log__:{\"n_iter\": 4, \"mean_cosine-nn-S2T-10000\": 0.7053986191749573, \"mean_cosine-csls_knn_10-S2T-10000\": 0.7037082314491272}\n",
            "INFO - 01/11/23 22:37:04 - 0:12:15 - End of refinement iteration 4.\n",
            "                                     \n",
            "                                     \n"
          ]
        }
      ],
      "source": [
        "!python unsupervised.py --src_lang fr --tgt_lang en --src_emb data/wiki.fr.vec --tgt_emb data/wiki.en.vec --n_epochs 1 --exp_name \"fr_en\" --exp_path \"./\" --export \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8dxX_AP5nbA"
      },
      "source": [
        "## Getting the mapping output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tVIq_MVQVOP7"
      },
      "outputs": [],
      "source": [
        "id = !ls fr_en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4jsYFYg-Vddu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "mapping_data = torch.load(\"fr_en/\" + id[0] + \"/best_mapping.pth\")\n",
        "mapping = torch.nn.Linear(300,300, bias=False)\n",
        "mapping.weight.data = torch.from_numpy(mapping_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnT9ujcj52uq"
      },
      "source": [
        "## Getting the datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RD_Rz9MEYqB3"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fr = KeyedVectors.load_word2vec_format('data/wiki.fr.vec', binary=False)\n",
        "en = KeyedVectors.load_word2vec_format('data/wiki.en.vec', binary=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTD00oGF58cz"
      },
      "source": [
        "## Testing the mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A0rH5RarVxrw"
      },
      "outputs": [],
      "source": [
        "words_fr = [\"beau\", \"clavier\", \"monsieur\", \"cheval\"]\n",
        "words_en = [\"beautiful\", \"keyboard\", \"mister\", \"horse\"]\n",
        "\n",
        "encoded_words_fr = [] \n",
        "encoded_words_en = [] \n",
        "\n",
        "for word in words_fr:\n",
        "  encoded_words_fr.append(fr[word])\n",
        "for word in words_en:\n",
        "  encoded_words_en.append(en[word])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aCutpbUEfgao",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f62ada8-1ffd-4f5e-cee0-221ab0425d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-954f0fa6b3af>:4: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
            "  mapped_words.append(mapping(torch.from_numpy(word)).detach().numpy())\n"
          ]
        }
      ],
      "source": [
        "mapped_words = []\n",
        "\n",
        "for word in encoded_words_fr:\n",
        "  mapped_words.append(mapping(torch.from_numpy(word)).detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MiQZviaR5Vma"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def reduce_to_k_dim(M, k=2):\n",
        "    n_iters = 10     \n",
        "    M_reduced = None\n",
        "\n",
        "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
        "    M_reduced = svd.fit_transform(M)\n",
        "\n",
        "    return M_reduced\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vizualizing the mapping result"
      ],
      "metadata": {
        "id": "k6MOPAIs1u9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "CNoYJK50Zurw",
        "outputId": "d96dbeee-db80-499c-cf20-4a6cdb1718a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd6216bfca0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "points = reduce_to_k_dim(encoded_words_fr + encoded_words_en)\n",
        "mapPoints = reduce_to_k_dim(mapped_words + encoded_words_en)\n",
        "labels = words_fr + words_en\n",
        "\n",
        "plt.scatter([el[0] for el in points], [el[1] for el in points], color=\"blue\", label=\"without mapping\")\n",
        "plt.scatter([el[0] for el in mapPoints], [el[1] for el in mapPoints], color=\"red\", label=\"with mapping\")\n",
        "\n",
        "\n",
        "for i, txt in enumerate(labels):\n",
        "    plt.annotate(txt, (points[i][0], points[i][1]+0.05))\n",
        "    plt.annotate(txt, (mapPoints[i][0], mapPoints[i][1]+0.05))\n",
        "\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdlZ5znG6r9P"
      },
      "source": [
        "## Apply the translation approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "KdygRK5FonLo"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def translation(word, points, mapPoints):\n",
        "    closest_distance = float('inf')\n",
        "    closest_point = None\n",
        "    for point in point_list:\n",
        "        distance = math.sqrt((point[0] - reference[0])**2 + (point[1] - reference[1])**2)\n",
        "        if distance < closest_distance:\n",
        "            closest_distance = distance\n",
        "            closest_point = point\n",
        "    return closest_point\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}